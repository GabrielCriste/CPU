<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Laika 0.18.1 + Helium Theme" />
    <title>Tensors</title>
    
      <meta name="author" content="Sören Brunk"/>
    
    
      <meta name="description" content="docs"/>
    
    
    
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css">
    
    <link rel="stylesheet" type="text/css" href="../helium/icofont.min.css" />
    <link rel="stylesheet" type="text/css" href="../helium/laika-helium.css" />
    <link rel="stylesheet" type="text/css" href="../site/styles.css" />
    <script src="../helium/laika-helium.js"></script>
    
    
    <script> /* for avoiding page load transitions */ </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
            ],
            // • rendering keys, e.g.:
            throwOnError : false
          });
      });
    </script>
  </head>

  <body>

    <header id="top-bar">

      <div class="row">
        <a id="nav-icon">
          <i class="icofont-laika" title="Navigation">&#xefa2;</i>
        </a>
        
      </div>

      <a class="icon-link" href="../index.html"><i class="icofont-laika" title="Home">&#xef47;</i></a>

      <span class="row links"><a class="icon-link svg-link" href="../api/index.html"><span title="API"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M75,47.5c13.246,0 24,10.754 24,24c0,13.246 -10.754,24 -24,24c-13.246,0 -24,-10.754 -24,-24c0,-13.246 10.754,-24 24,-24Zm-50,-0c13.246,-0 24,10.754 24,24c0,13.246 -10.754,24 -24,24c-13.246,-0 -24,-10.754 -24,-24c0,-13.246 10.754,-24 24,-24Zm2.705,16.735l7.239,0l0.622,-4.904l-21.833,0l-0,4.904l7.589,0l0,22.067l6.383,0l-0,-22.067Zm58.076,7.265c-0,-8.757 -3.698,-14.166 -10.781,-14.166c-7.083,-0 -10.781,5.604 -10.781,14.166c0,8.757 3.698,14.166 10.781,14.166c7.083,0 10.781,-5.604 10.781,-14.166Zm-6.539,0c0,6.538 -1.128,9.496 -4.242,9.496c-2.997,0 -4.242,-2.88 -4.242,-9.496c-0,-6.616 1.206,-9.496 4.242,-9.496c3.036,-0 4.242,2.88 4.242,9.496Zm-29.242,-67c13.246,0 24,10.754 24,24c0,13.246 -10.754,24 -24,24c-13.246,0 -24,-10.754 -24,-24c0,-13.246 10.754,-24 24,-24Zm0.512,9.834c-7.122,-0 -12.609,5.098 -12.609,14.127c-0,9.263 5.215,14.205 12.532,14.205c4.164,0 7.083,-1.634 9.068,-3.658l-2.88,-3.697c-1.518,1.206 -3.153,2.413 -5.838,2.413c-3.697,-0 -6.266,-2.763 -6.266,-9.263c-0,-6.616 2.724,-9.379 6.149,-9.379c2.102,-0 3.892,0.778 5.371,1.984l3.113,-3.775c-2.257,-1.868 -4.748,-2.957 -8.64,-2.957Z"/>
  </g>
</svg></span></a><a class="icon-link svg-link" href="https://github.com/sbrunk/storch"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a></span>

    </header>

    <nav id="sidebar">

      <div class="row">
        <a class="icon-link svg-link" href="../api/index.html"><span title="API"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M75,47.5c13.246,0 24,10.754 24,24c0,13.246 -10.754,24 -24,24c-13.246,0 -24,-10.754 -24,-24c0,-13.246 10.754,-24 24,-24Zm-50,-0c13.246,-0 24,10.754 24,24c0,13.246 -10.754,24 -24,24c-13.246,-0 -24,-10.754 -24,-24c0,-13.246 10.754,-24 24,-24Zm2.705,16.735l7.239,0l0.622,-4.904l-21.833,0l-0,4.904l7.589,0l0,22.067l6.383,0l-0,-22.067Zm58.076,7.265c-0,-8.757 -3.698,-14.166 -10.781,-14.166c-7.083,-0 -10.781,5.604 -10.781,14.166c0,8.757 3.698,14.166 10.781,14.166c7.083,0 10.781,-5.604 10.781,-14.166Zm-6.539,0c0,6.538 -1.128,9.496 -4.242,9.496c-2.997,0 -4.242,-2.88 -4.242,-9.496c-0,-6.616 1.206,-9.496 4.242,-9.496c3.036,-0 4.242,2.88 4.242,9.496Zm-29.242,-67c13.246,0 24,10.754 24,24c0,13.246 -10.754,24 -24,24c-13.246,0 -24,-10.754 -24,-24c0,-13.246 10.754,-24 24,-24Zm0.512,9.834c-7.122,-0 -12.609,5.098 -12.609,14.127c-0,9.263 5.215,14.205 12.532,14.205c4.164,0 7.083,-1.634 9.068,-3.658l-2.88,-3.697c-1.518,1.206 -3.153,2.413 -5.838,2.413c-3.697,-0 -6.266,-2.763 -6.266,-9.263c-0,-6.616 2.724,-9.379 6.149,-9.379c2.102,-0 3.892,0.778 5.371,1.984l3.113,-3.775c-2.257,-1.868 -4.748,-2.957 -8.64,-2.957Z"/>
  </g>
</svg></span></a><a class="icon-link svg-link" href="https://github.com/sbrunk/storch"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a>
      </div>

      <ul class="nav-list">
        <li class="level1"><a href="../about.html">About</a></li>
        <li class="level1"><a href="../installation.html">Installation</a></li>
        <li class="level1"><a href="../modules.html">Modules</a></li>
        <li class="level1 nav-header">tutorial</li>
        <li class="level2 active"><a href="#">Tensors</a></li>
        <li class="level2"><a href="buildmodel.html">Build the Neural Network</a></li>
        <li class="level2"><a href="autograd.html">Automatic Differentiation</a></li>
      </ul>

      <ul class="nav-list">
        <li class="level1 nav-header">Related Projects</li>
        
          <li class="level2"><a href="https://pytorch.org/">PyTorch</a></li>
        
          <li class="level2"><a href="https://github.com/bytedeco/javacpp">JavaCPP</a></li>
        
      </ul>

    </nav>

    <div id="container">

      <nav id="page-nav">
        <p class="header"><a href="#">Tensors</a></p>

        <ul class="nav-list">
          <li class="level1"><a href="#initializing-a-tensor">Initializing a Tensor</a></li>
          <li class="level1"><a href="#attributes-of-a-tensor">Attributes of a Tensor</a></li>
          <li class="level1"><a href="#operations-on-tensors">Operations on Tensors</a></li>
        </ul>

        <p class="footer"></p>
      </nav>

      <main class="content">

        <p>Learn the Basics] ||
        Quickstart ||
        <strong>Tensors</strong> ||
        Datasets &amp; DataLoaders ||
        Transforms ||
        <a href="buildmodel.html">Build Model</a> ||
        <a href="autograd.html">Autograd</a> ||
        Optimization ||
        Save &amp; Load Model</p>
        <h1 id="tensors" class="title">Tensors</h1>
        <p>Tensors are a specialized data structure that are very similar to arrays
        and matrices. In PyTorch, we use tensors to encode the inputs and
        outputs of a model, as well as the model’s parameters.</p>
        <p>Tensors are similar to <a href="https://numpy.org/">NumPy’s</a> ndarrays, except
        that tensors can run on GPUs or other hardware accelerators. Tensors
        are also optimized for automatic differentiation (we&#39;ll see more about
        that later in the <a href="autograd.html">Autograd</a> section). If
        you’re familiar with ndarrays, you’ll be right at home with the Tensor
        API. If not, follow along!</p>
        
        <h2 id="initializing-a-tensor" class="section">Initializing a Tensor<a class="anchor-link right" href="#initializing-a-tensor"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Tensors can be initialized in various ways. Take a look at the following
        examples:</p>
        <p><strong>Directly from data</strong></p>
        <p>Tensors can be created directly from data. The data type is
        automatically inferred.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">data</span><span> = </span><span class="type-name">Seq</span><span>(</span><span class="number-literal">1</span><span>, </span><span class="number-literal">2</span><span>, </span><span class="number-literal">3</span><span>, </span><span class="number-literal">4</span><span>)
</span><span class="comment">// data: Seq[Int] = List(1, 2, 3, 4)
</span><span class="keyword">val</span><span> </span><span class="identifier">xData</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="type-name">Tensor</span><span>(</span><span class="identifier">data</span><span>).</span><span class="identifier">reshape</span><span>(</span><span class="number-literal">2</span><span>,</span><span class="number-literal">2</span><span>)
</span><span class="comment">// xData: Tensor[Int32] = tensor dtype=int32, shape=[2, 2], device=CPU 
// [[1, 2],
//  [3, 4]]</span></code></pre>
        <p><strong>From another tensor:</strong></p>
        <p>The new tensor retains the properties (shape, datatype) of the argument
        tensor, unless explicitly overridden.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="comment">// Ones Tensor:
</span><span class="keyword">val</span><span> </span><span class="identifier">xOnes</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="identifier">onesLike</span><span>(</span><span class="identifier">xData</span><span>) </span><span class="comment">// retains the properties of xData
// xOnes: Tensor[Int32] = tensor dtype=int32, shape=[2, 2], device=CPU 
// [[1, 1],
//  [1, 1]]</span></code></pre>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="comment">// Random Tensor:
</span><span class="keyword">val</span><span> </span><span class="identifier">xRand</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="identifier">randLike</span><span>(</span><span class="identifier">xData</span><span>, </span><span class="identifier">dtype</span><span>=</span><span class="identifier">torch</span><span>.</span><span class="identifier">float32</span><span>) </span><span class="comment">// overrides the datatype of xData
// xRand: Tensor[Float32] = tensor dtype=float32, shape=[2, 2], device=CPU 
// [[0.4963, 0.7682],
//  [0.0885, 0.1320]]</span></code></pre>
        <p><strong>With random or constant values:</strong></p>
        <p><code>shape</code> is a tuple of tensor dimensions. In the functions below, it
        determines the dimensionality of the output tensor.</p>
        <pre><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">shape</span><span> = </span><span class="type-name">Seq</span><span>(</span><span class="number-literal">2</span><span>,</span><span class="number-literal">3</span><span>)
</span><span class="comment">// shape: Seq[Int] = List(2, 3)
</span><span>
</span><span class="comment">// Random Tensor:
</span><span class="keyword">val</span><span> </span><span class="identifier">randTensor</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="identifier">rand</span><span>(</span><span class="identifier">shape</span><span>)
</span><span class="comment">// randTensor: Tensor[Float32] = tensor dtype=float32, shape=[2, 3], device=CPU 
// [[0.3074, 0.6341, 0.4901],
//  [0.8964, 0.4556, 0.6323]]
</span><span>
</span><span class="comment">// Ones Tensor: 
</span><span class="keyword">val</span><span> </span><span class="identifier">onesTensor</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="identifier">ones</span><span>(</span><span class="identifier">shape</span><span>)
</span><span class="comment">// onesTensor: Tensor[Float32] = tensor dtype=float32, shape=[2, 3], device=CPU 
// [[1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000]]
</span><span>
</span><span class="comment">// Zeros Tensor:
</span><span class="keyword">val</span><span> </span><span class="identifier">zerosTensor</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="identifier">zeros</span><span>(</span><span class="identifier">shape</span><span>)
</span><span class="comment">// zerosTensor: Tensor[Float32] = tensor dtype=float32, shape=[2, 3], device=CPU 
// [[0.0000, 0.0000, 0.0000],
//  [0.0000, 0.0000, 0.0000]]</span></code></pre>
        
        <h2 id="attributes-of-a-tensor" class="section">Attributes of a Tensor<a class="anchor-link right" href="#attributes-of-a-tensor"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Tensor attributes describe their shape, datatype, and the device on
        which they are stored.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">var</span><span> </span><span class="identifier">tensor</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="identifier">rand</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="number-literal">3</span><span>,</span><span class="number-literal">4</span><span>))
</span><span class="comment">// tensor: Tensor[Float32] = tensor dtype=float32, shape=[3, 4], device=CPU 
// [[0.3489, 0.4017, 0.0223, 0.1689],
//  [0.2939, 0.5185, 0.6977, 0.8000],
//  [0.1610, 0.2823, 0.6816, 0.9152]]
</span><span>
</span><span class="identifier">println</span><span>(</span><span class="string-literal">s&quot;Shape of tensor: </span><span class="substitution">${tensor.shape}</span><span class="string-literal">&quot;</span><span>)
</span><span class="comment">// Shape of tensor: ArraySeq(3, 4)
</span><span class="identifier">println</span><span>(</span><span class="string-literal">s&quot;Datatype of tensor: </span><span class="substitution">${tensor.dtype}</span><span class="string-literal">&quot;</span><span>)
</span><span class="comment">// Datatype of tensor: float32
</span><span class="identifier">println</span><span>(</span><span class="string-literal">s&quot;Device tensor is stored on: {tensor.device}&quot;</span><span>)
</span><span class="comment">// Device tensor is stored on: {tensor.device}</span></code></pre>
        <hr>
        
        <h2 id="operations-on-tensors" class="section">Operations on Tensors<a class="anchor-link right" href="#operations-on-tensors"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Over 100 tensor operations, including arithmetic, linear algebra, matrix
        manipulation (transposing, indexing, slicing), sampling and more are
        comprehensively described
        <a href="https://pytorch.org/docs/stable/torch.html">here</a>.</p>
        <p>Each of these operations can be run on the GPU (at typically higher
        speeds than on a CPU). If you’re using Colab, allocate a GPU by going to
        Runtime &gt; Change runtime type &gt; GPU.</p>
        <p>By default, tensors are created on the CPU. We need to explicitly move
        tensors to the GPU using <code>.to</code> method (after checking for GPU
        availability). Keep in mind that copying large tensors across devices
        can be expensive in terms of time and memory! We move our tensor to the
        GPU if available</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">if</span><span> </span><span class="identifier">torch</span><span>.</span><span class="identifier">cuda</span><span>.</span><span class="identifier">isAvailable</span><span> </span><span class="keyword">then</span><span>  
  </span><span class="identifier">tensor</span><span> = </span><span class="identifier">tensor</span><span>.</span><span class="identifier">to</span><span>(</span><span class="identifier">torch</span><span>.</span><span class="type-name">Device</span><span>.</span><span class="type-name">CUDA</span><span>)</span></code></pre>
        <p>Try out some of the operations from the list. If you&#39;re familiar with
        the NumPy API, you&#39;ll find the Tensor API a breeze to use.</p>
        <p><strong>Standard numpy-like indexing and slicing:</strong></p>
        <pre><code class="nohighlight"><span class="keyword">import</span><span> </span><span class="identifier">torch</span><span>.{---, </span><span class="type-name">Slice</span><span>}
</span><span class="identifier">tensor</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="identifier">ones</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="number-literal">4</span><span>, </span><span class="number-literal">4</span><span>))
</span><span class="identifier">println</span><span>(</span><span class="string-literal">s&quot;First row: </span><span class="substitution">${tensor(0)}</span><span class="string-literal">&quot;</span><span>)
</span><span class="comment">// First row: tensor dtype=float32, shape=[4], device=CPU 
// [1.0000, 1.0000, 1.0000, 1.0000]
</span><span class="identifier">println</span><span>(</span><span class="string-literal">s&quot;First column: </span><span class="substitution">${tensor(Slice(), 0)}</span><span class="string-literal">&quot;</span><span>)
</span><span class="comment">// First column: tensor dtype=float32, shape=[4], device=CPU 
// [1.0000, 1.0000, 1.0000, 1.0000]
</span><span class="identifier">println</span><span>(</span><span class="string-literal">s&quot;Last column: </span><span class="substitution">${tensor(---, -1)}</span><span class="string-literal">&quot;</span><span>)
</span><span class="comment">// Last column: tensor dtype=float32, shape=[4], device=CPU 
// [1.0000, 1.0000, 1.0000, 1.0000]
//tensor(---,1) = 0 TODO update op
</span><span class="identifier">println</span><span>(</span><span class="identifier">tensor</span><span>)
</span><span class="comment">// tensor dtype=float32, shape=[4, 4], device=CPU 
// [[1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000]]</span></code></pre>
        <p><strong>Joining tensors</strong> You can use <code>torch.cat</code> to concatenate a sequence of
        tensors along a given dimension. See also
        <a href="https://pytorch.org/docs/stable/generated/torch.stack.html">torch.stack</a>,
        another tensor joining op that is subtly different from <code>torch.cat</code>.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">t1</span><span> = </span><span class="identifier">torch</span><span>.</span><span class="identifier">cat</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="identifier">tensor</span><span>, </span><span class="identifier">tensor</span><span>, </span><span class="identifier">tensor</span><span>), </span><span class="identifier">dim</span><span>=</span><span class="number-literal">1</span><span>)
</span><span class="comment">// t1: Tensor[Float32] = tensor dtype=float32, shape=[4, 12], device=CPU 
// [[1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000]]
</span><span class="identifier">println</span><span>(</span><span class="identifier">t1</span><span>)
</span><span class="comment">// tensor dtype=float32, shape=[4, 12], device=CPU 
// [[1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000]]</span></code></pre>
        <p><strong>Arithmetic operations</strong></p>
        <pre><code class="nohighlight"><span class="comment">// This computes the matrix multiplication between two tensors. y1, y2, y3 will
// have the same value
// `tensor.mT` returns the transpose of a tensor
</span><span class="keyword">val</span><span> </span><span class="identifier">y1</span><span> = </span><span class="identifier">tensor</span><span> </span><span class="identifier">`@`</span><span> </span><span class="identifier">tensor</span><span>.</span><span class="identifier">mT</span><span>
</span><span class="comment">// y1: Tensor[Float32] = tensor dtype=float32, shape=[4, 4], device=CPU 
// [[4.0000, 4.0000, 4.0000, 4.0000],
//  [4.0000, 4.0000, 4.0000, 4.0000],
//  [4.0000, 4.0000, 4.0000, 4.0000],
//  [4.0000, 4.0000, 4.0000, 4.0000]]
</span><span class="keyword">val</span><span> </span><span class="identifier">y2</span><span> = </span><span class="identifier">tensor</span><span>.</span><span class="identifier">matmul</span><span>(</span><span class="identifier">tensor</span><span>.</span><span class="identifier">mT</span><span>)
</span><span class="comment">// y2: Tensor[Float32] = tensor dtype=float32, shape=[4, 4], device=CPU 
// [[4.0000, 4.0000, 4.0000, 4.0000],
//  [4.0000, 4.0000, 4.0000, 4.0000],
//  [4.0000, 4.0000, 4.0000, 4.0000],
//  [4.0000, 4.0000, 4.0000, 4.0000]]
</span><span>
</span><span class="comment">//val y3 = torch.randLike(y1)
//torch.matmul(tensor, tensor.mT, out=y3)
</span><span>
</span><span class="comment">// This computes the element-wise product. z1, z2, z3 will have the same value
</span><span>
</span><span class="keyword">val</span><span> </span><span class="identifier">z1</span><span> = </span><span class="identifier">tensor</span><span> * </span><span class="identifier">tensor</span><span>
</span><span class="comment">// z1: Tensor[Float32] = tensor dtype=float32, shape=[4, 4], device=CPU 
// [[1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000]]
</span><span class="keyword">val</span><span> </span><span class="identifier">z2</span><span> = </span><span class="identifier">tensor</span><span>.</span><span class="identifier">mul</span><span>(</span><span class="identifier">tensor</span><span>)
</span><span class="comment">// z2: Tensor[Float32] = tensor dtype=float32, shape=[4, 4], device=CPU 
// [[1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000]]</span></code></pre>
        <p><strong>Single-element tensors</strong> If you have a one-element tensor, for example
        by aggregating all values of a tensor into one value, you can convert it
        to a Scala numerical value using <code>item()</code>:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">agg</span><span> = </span><span class="identifier">tensor</span><span>.</span><span class="identifier">sum</span><span>
</span><span class="comment">// agg: Tensor[Float32] = tensor dtype=float32, shape=[], device=CPU 
// 16.0000
</span><span class="keyword">val</span><span> </span><span class="identifier">aggItem</span><span> = </span><span class="identifier">agg</span><span>.</span><span class="identifier">item</span><span>
</span><span class="comment">// aggItem: Float = 16.0F
</span><span class="identifier">print</span><span>(</span><span class="identifier">aggItem</span><span>)
</span><span class="comment">// 16.0
</span><span class="identifier">println</span><span>(</span><span class="identifier">aggItem</span><span>.</span><span class="identifier">getClass</span><span>)
</span><span class="comment">// float</span></code></pre>
        <p><strong>In-place operations</strong> Operations that store the result into the
        operand are called in-place. They are denoted by a <code>_</code> suffix. For
        example: <code>x.copy_(y)</code>, <code>x.t_()</code>, will change <code>x</code>.</p>
        <pre><code class="nohighlight"><span class="identifier">println</span><span>(</span><span class="string-literal">s&quot;</span><span class="substitution">$tensor</span><span class="string-literal">&quot;</span><span>)
</span><span class="comment">// tensor dtype=float32, shape=[4, 4], device=CPU 
// [[1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000],
//  [1.0000, 1.0000, 1.0000, 1.0000]]
</span><span class="identifier">tensor</span><span> -= </span><span class="number-literal">5</span><span>
</span><span class="comment">// res14: Tensor[Float32] = tensor dtype=float32, shape=[4, 4], device=CPU 
// [[-4.0000, -4.0000, -4.0000, -4.0000],
//  [-4.0000, -4.0000, -4.0000, -4.0000],
//  [-4.0000, -4.0000, -4.0000, -4.0000],
//  [-4.0000, -4.0000, -4.0000, -4.0000]]
</span><span class="identifier">println</span><span>(</span><span class="identifier">tensor</span><span>)
</span><span class="comment">// tensor dtype=float32, shape=[4, 4], device=CPU 
// [[-4.0000, -4.0000, -4.0000, -4.0000],
//  [-4.0000, -4.0000, -4.0000, -4.0000],
//  [-4.0000, -4.0000, -4.0000, -4.0000],
//  [-4.0000, -4.0000, -4.0000, -4.0000]]</span></code></pre>
        <div class="callout info">
          <i class="icofont-laika">&#xef4e;</i>
          <p>In-place operations save some memory, but can be problematic when
          computing derivatives because of an immediate loss of history. Hence,
          their use is discouraged.</p>
        </div>

      </main>

    </div>

  </body>
</html>
